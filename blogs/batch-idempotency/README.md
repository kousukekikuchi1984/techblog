## Introduction

本記事では、ある実プロジェクトで設計したOLTP系の大規模バッチ処理について、その設計思想と背景、実装上の工夫、トレードオフなどを記録します。

対象となるデータは不可逆かつ時間制約のあるもので、処理ミスが許されない環境下にあります。この記事が、同様の状況に直面しているエンジニアやアーキテクトの参考になれば幸いです。

## Philosophy

### 人は間違える
- 当たり前と言えば当たり前ですが、まず人が作っている以上、システムは間違えることがあります。
- これは自分もそうだし、先方のシステムもそうです。
- そのため、前提として「データは必ずどこかにミスがある」ことを踏まえて設計し、ミスを安全に排除できるようにしておく必要があります。

### 直観に裏付けられた恐怖は正しい
- ここでいう「直観」は、論理的な証明ではなく、経験に裏付けられた危機感を指します。
- ソフトウェアエンジニアにとって恐怖は設計時に重要です。潜在的な危険を察知するセンサーとして働きます。
- 特に一度投入したら戻せないバッチ処理においては、「怖いと感じる箇所」こそ丁寧に設計する価値があります。

## Requirements

### データや制約について
- データ生成元はメインフレームで動く重厚なシステムで、日次で大規模なCSVファイルが送られてきます。
- 想定されるファイルサイズは数百万レコード、数GB程度。
- 外部由来である以上、先方のミスも一定割合で存在します。
- 本処理は、アップロード後1時間以内に完了することが求められています。
- 当方のシステムはOLTPであるため、即時性・整合性・冪等性が求められます。
- さらに、QAチームがこのバッチの結果を元に検証を行うため、彼らの作業が滞らないよう配慮も必要です。

## 設計に至るまでの経験

### 綺麗なデータは存在しない
- データ連携においても、内部ですら誤ったデータが流れてくるケースがありました。
- 外部システムからの連携で、しかもOLTPのように即時投入・取り消し不可のケースでは、これは致命的です。
- よって、怪しいデータは確実に排除する仕組みが必要であり、それを事前に想定しておくのが現実的な態度です。

### 冪等性の担保
- 過去にデータエンジニアとして働いていた際、リトライ可能なバッチとそうでないものの運用負荷の差を強く感じました。
- 一度失敗したとしても、再実行時に正しく状態を整合できるように設計しておくことは、将来の保守性と信頼性を大きく左右します。
- 冪等性を担保できたことで、運用者やQAチームからの信頼も厚くなりました。

## High-level Design

- 通常のシステムであれば、ユーザー単位やレコード単位で逐次処理をすることが一般的です。
- しかし本件では並列性と安全性の両立が必要だったため、すべてのデータを「フェーズ」ごとに分離しました。
- 全データに対して「前処理 → バリデーション → データ投入」の順で全件処理を完了するまで、次のフェーズに進まない設計にしています。

## Design Deep Dive

### 前処理
- duckdb を用いてCSVを取り込み、処理単位（例えばシャードやID範囲）ごとに分割。
- その結果を並列処理可能な単位として分配。

### バリデーション
- データ仕様に照らして不正なレコードを検出し、エラー時はバッチ全体を即時中止します。
- 仕様に曖昧さがある場合でも、読み取れる限り厳密にルールを設け、事前に排除するようにしています。

### データ投入
- バリデーションを通過したデータのみを投入します。
- ただし、validation通過から投入までの間に対象データが変化する可能性（例：ユーザー統合、削除）もあるため、投入時に必要なロック・整合チェックを行います。

### ログ・リトライ方針
- すべての処理において、処理単位ごとに一意なIDを付与し、成功／失敗状態を明確に記録。
- リトライ時はそのIDと処理状態を元に、再投入・スキップ・エラー通知を制御します。

## トレードオフと判断理由

### Reverse ETL vs 単一バッチ処理
- 当初は、DWHで使われるReverse ETL的アーキテクチャを検討しました。
- しかし、以下の理由により単一バッチ構成としました：
  - QAチームが複数の中間テーブルを検証するのが現実的でない
  - Validationをどこで行うか問題（最終的にはどこかが責任を持たなければならない）
  - DWHはinfraチームの所管であり、構築コストと権限面の制約がある
  - duckdbでDWH的な一時処理空間を内部で確保できた

### 実装の複雑さ
- 並列処理の前段階として、ID分割・中間管理構造などの実装を新規に行う必要がありました。
- Validationルールの形式化も一筋縄ではいかず、数多くの仕様とにらめっこしながらの設計となりました。
- データ投入時の状態変化への考慮も必要で、ここは最も複雑でかつ責任の重い箇所となっています。

## 結果として

### 並列化の効果
- 当初の目標は「1時間以内に完了すること」でしたが、現在はおおむね5分程度で完了しています。
- 設計段階で十分なスケーラビリティを確保したことで、将来的なデータ増加にも耐えられる構成になっています。

### 実際に運用して
- 運用中、実際に先方から誤ったデータが送信されたことがありました。
- それを事前にバリデーションで確実に排除できたことで、システム障害やロールバック不能な状況を防げました。
- この設計が功を奏した具体的な成功例となっています。

## まとめ・今後の展望

本バッチ処理設計は、私自身が「怖い」と感じたポイントを出発点に構築したものです。  
その感覚は決して感情論ではなく、過去の失敗や運用課題から培われた直観でした。
