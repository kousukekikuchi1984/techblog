## Introduction

## Philosophy
### 人は間違える
- 当たり前と言えば当たり前
- まず人が作っている以上、システムは間違えることがある。
- これは自分もそうだが、先方のシステムも間違える
- この間違いを適切に落とさないといけない

### 直観に裏付けられた恐怖は正しい
- 直感ではなく直観
- ソフトウェアエンジニアにとって恐怖は設計時に重要。危険な箇所を教えてくれる。

## Requirements
### データや制約について
- データ生成元はメインフレームで重厚なシステムでそこから大規模データが CSV 形式で送られてくる
- とはいえ、先方のミスも想定する必要がある。
- このデータ連携の制約上、1時間以内で処理しないといけない
- こちらのシステムは OLTP である。
- こちらには QA チームがいるため、彼らの検証中に手間を取らせないこと。

## 設計に至るまでの経験
### 綺麗なデータは存在しない
- データ連携においても相当誤ったデータが届く
- 内部連携ですらそうなのに外部連携、しかも OLTP の場合は引き返しができない。
- それのみならず、
- 怪しいデータは確実に落とすべきという発想

### 冪等性の担保
- データエンジニアとして働いていた経験として冪等は重要
- 何度もリトライできるため。
- 結果として、運用可能性を向上することができた経験がある。
- 逆にそれが担保されていないものは後の運用難易度が跳ね上がった

## High-level Design

- 普通の処理はユーザーごとに処理したり、連携データを一行ごとに処理する例が多い。
- が、これは並列処理と安全性の観点からできない
- そのため、全データをフェーズごとに分割して前処理、バリデーション、データ投入に分割して全てのデータが通過するまで次へ進まないようにしました。

## Design Deep Dive
### 前処理
- duckdb を使ってデータを並列処理の単位にまとめる。
- これを並列処理に流す

### バリデーション
- 思想的にはデータ由来の問題はここで全て落としていて、通過したものは確実にデータ投入を落とすようにしています。
- 先方のデータ仕様にしたがって確実に誤っているものに関しては確実にバッチを落とす
  - と書くとものすごくシンプルに見えますが、これは読みとけるものに関してはがんじがらめにかけています。

### データ投入
- ここでは validation を通過したデータを確実に投入することが目的です。
- と書くとものすごくシンプルですが
  - 実際は、validation の間にデータが更新されていたり、アカウント統合でユーザーが消えていたりする
- 更新されている可能性があるデータは lock をかけながら適切に調整する。

### ログ・リトライ方針

## トレードオフと判断理由
### Reverse ETL vs 単一バッチ処理
- 当初は DWH で用いられるような Reverse ETL と同じ設計にしようとしたものの下記の理由で却下
  - QA チームの検証工数が高くなる
- どこかで Validation をする必要があり、その Validation は最終的にどこかで持たなくてはならない
  - Reverse ETL の場合は、投入前のテーブルを用意し、
- DWH は infra チームの持ち物であり、自チームには DWH は簡易的なものも含めないため、構築コストが大きい
  - なお内部的には duckdb を使っており、ここで DWH と同様なことをしている。

### 実装の複雑さ
- 並列処理やその前処理といった仕組みをゼロから実装しなくてはならなかった。
- validation も複雑
- データ投入箇所も複雑


## 結果として

### 並列化の効果
- 目標1時間のところ、概ね5分くらいで処理できている。
- 過度な高速化であったかもしれない。

### 実際に運用して
- 先方由来で誤ったデータが送られてきたことがあった
- これを取り込んでいたら二度と戻せないので、適切に落とせてよかった。

## まとめ・今後の展望

- 今まで経験したことが要件定義時に怖いなと思ったことをなんとか回避するように設計を進めました
- こういうように直観というのは重要という話
